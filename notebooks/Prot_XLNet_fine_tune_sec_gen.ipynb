{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43334558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aae9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar wandb completamente\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a3d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available devices: 1\n",
      "current device: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'available devices: {torch.cuda.device_count()}')\n",
    "print(f'current device: { torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0b790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay GPU disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f49430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros para generación de secuencias\n",
    "model_name = \"Rostlab/prot_xlnet\"\n",
    "batch_size = 8  # Reducido para generación que requiere más memoria\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "max_length = 512\n",
    "output_dir = \"./prot_xlnet_generation_finetuned_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5db36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida si no existe\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466589d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando tokenizador y modelo para generación de secuencias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alejandros\\Documents\\GitHub\\Advanced_drug_discovery\\.venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "# Cargar tokenizador y modelo para generación de lenguaje\n",
    "print(\"Cargando tokenizador y modelo para generación de secuencias...\")\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "# Usar XLNetLMHeadModel para generación de texto/secuencias\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f488d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario del tokenizador: 37 tokens\n",
      "El modelo está configurado para generación de secuencias de proteínas\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulario del tokenizador: {tokenizer.vocab_size} tokens\")\n",
    "print(\"El modelo está configurado para generación de secuencias de proteínas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7f0db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetLMHeadModel(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(37, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0-29): 30 x XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): ReLU()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_loss): Linear(in_features=1024, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mover el modelo al dispositivo adecuado\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fecc8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para congelar las capas base y solo entrenar las nuevas\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da424da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear datos de ejemplo (reemplazar con secuencias reales)\n",
    "def create_protein_sequences_data():\n",
    "    \"\"\"\n",
    "    Crea un conjunto de datos de ejemplo de secuencias de proteínas.\n",
    "    REEMPLAZA ESTA FUNCIÓN con la carga de tus secuencias reales.\n",
    "    \"\"\"\n",
    "    # Secuencias de ejemplo más realistas\n",
    "    sequences = [\n",
    "        \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "        \"MASNTVSAQGGSNRPVRDLASRQDFVRASSIIEKQLRDKVSADDLPVTLAQHLAVNFLHVLRLLE\",\n",
    "        \"MTMDKSELVQKAKLAEQAERYDDMAAAMKAVTEQGHELSNEERNLLSVAYKNVVGARRSSWRVVSSIEQKTEGAEKKQQ\",\n",
    "        \"MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDLQAKGIQYLHQKGLKKQVPEGVKVP\",\n",
    "        \"MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNLHGLFGRKTGQAPGYSYTAANKNKGIIWGEDTLMEYLENPKKYIPGTKMIFVGIKKKEERADLIAYLKKATNE\"\n",
    "    ]\n",
    "    \n",
    "    # Añadir más secuencias simuladas si necesitas más datos de prueba\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    for i in range(50):  # Reducido para este ejemplo\n",
    "        seq_length = np.random.randint(80, 300)  # Longitudes más realistas\n",
    "        seq = ''.join(np.random.choice(list(amino_acids)) for _ in range(seq_length))\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b581f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_protein_sequences(file_path=None):\n",
    "    \"\"\"\n",
    "    Función para cargar tus secuencias reales de proteínas.\n",
    "    Adapta esta función según el formato de tus datos.\n",
    "    \n",
    "    Formatos soportados:\n",
    "    - CSV con columna 'sequence'\n",
    "    - FASTA\n",
    "    - Texto plano (una secuencia por línea)\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        print(\"Usando datos de ejemplo. Para usar tus datos reales:\")\n",
    "        print(\"1. Modifica la función load_real_protein_sequences()\")\n",
    "        print(\"2. O pasa el path a tu archivo de secuencias\")\n",
    "        return create_protein_sequences_data()\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'sequence' in df.columns:\n",
    "            sequences = df['sequence'].tolist()\n",
    "        else:\n",
    "            print(\"El CSV debe tener una columna llamada 'sequence'\")\n",
    "            return create_protein_sequences_data()\n",
    "    \n",
    "    elif file_path.endswith('.fasta') or file_path.endswith('.fa'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            current_seq = \"\"\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                        current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line.strip()\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "    \n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            sequences = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Cargadas {len(sequences)} secuencias de {file_path}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0fa0fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargadas 6048 secuencias de ../data/raw/peptides_cancer.csv\n",
      "Total de secuencias cargadas: 6048\n",
      "Longitud promedio: 17.3 aminoácidos\n",
      "Rango de longitudes: 2 - 97\n"
     ]
    }
   ],
   "source": [
    "# Cargar secuencias (modifica el path para usar tus datos reales)\n",
    "#sequences = load_real_protein_sequences()  # Cambia por \n",
    "sequences = load_real_protein_sequences(\"../data/raw/peptides_cancer.csv\")\n",
    "\n",
    "print(f\"Total de secuencias cargadas: {len(sequences)}\")\n",
    "print(f\"Longitud promedio: {np.mean([len(seq) for seq in sequences]):.1f} aminoácidos\")\n",
    "print(f\"Rango de longitudes: {min(len(seq) for seq in sequences)} - {max(len(seq) for seq in sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9667526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25798212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar las secuencias para generación\n",
    "def tokenize_for_generation(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza las secuencias para entrenamiento de generación de lenguaje.\n",
    "    Añade tokens especiales y prepara para masked language modeling.\n",
    "    \"\"\"\n",
    "    # Tokenizar las secuencias\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    \n",
    "    # Para XLNet, usamos los input_ids como labels también (language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae9653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets\n",
    "train_dataset = Dataset.from_dict({\"sequence\": train_sequences})\n",
    "val_dataset = Dataset.from_dict({\"sequence\": val_sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc50c773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d210c2ca8e4dd59d94ac4a9ec6a678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda2f8797af045a199d5ea554036a114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizar los datasets\n",
    "train_dataset = train_dataset.map(tokenize_for_generation, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_for_generation, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d614b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el data collator para language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # XLNet usa autoregressive LM, no masked LM\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdcc305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alejandros\\Documents\\GitHub\\Advanced_drug_discovery\\.venv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=4,  # Aumentado para compensar batch size menor\n",
    "    report_to=None,\n",
    "    disable_tqdm=False,\n",
    "    prediction_loss_only=True,  # Solo calculamos loss para generación\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ccc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir función de métricas para generación\n",
    "def compute_generation_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas específicas para generación de secuencias.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Para generación, nos enfocamos principalmente en la perplejidad (calculada del loss)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d039e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquitectura del modelo:\n",
      "- Tipo: XLNetLMHeadModel (para generación de secuencias)\n",
      "- Parámetros: 409,413,669\n",
      "- Vocabulario: 37 tokens\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el Trainer para generación\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_generation_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nArquitectura del modelo:\")\n",
    "print(f\"- Tipo: {type(model).__name__} (para generación de secuencias)\")\n",
    "print(f\"- Parámetros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"- Vocabulario: {tokenizer.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9cf2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento para generación de secuencias...\n",
      "Este modelo aprenderá a generar secuencias de proteínas similares a las del entrenamiento\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3281e60ddc4dce83f31ed65bfeed1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.6694, 'grad_norm': 2.5340099334716797, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 12.7472, 'grad_norm': 2.5389554500579834, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.13}\n",
      "{'loss': 12.6117, 'grad_norm': 2.534273862838745, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 12.744, 'grad_norm': 2.530470848083496, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.26}\n",
      "{'loss': 12.798, 'grad_norm': 2.5221543312072754, 'learning_rate': 5e-06, 'epoch': 0.33}\n",
      "{'loss': 12.7354, 'grad_norm': 2.526870012283325, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 12.6937, 'grad_norm': 2.5525267124176025, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.46}\n",
      "{'loss': 12.7087, 'grad_norm': 2.5521256923675537, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.53}\n",
      "{'loss': 12.8252, 'grad_norm': 2.52350115776062, 'learning_rate': 9e-06, 'epoch': 0.6}\n",
      "{'loss': 12.795, 'grad_norm': 2.5364279747009277, 'learning_rate': 1e-05, 'epoch': 0.66}\n",
      "{'loss': 12.7503, 'grad_norm': 2.531712293624878, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.73}\n",
      "{'loss': 12.722, 'grad_norm': 2.5665271282196045, 'learning_rate': 1.2e-05, 'epoch': 0.79}\n",
      "{'loss': 12.8238, 'grad_norm': 2.5247340202331543, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.86}\n",
      "{'loss': 12.8905, 'grad_norm': 2.5279407501220703, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.93}\n",
      "{'loss': 12.6654, 'grad_norm': 2.529435634613037, 'learning_rate': 1.5e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1ac753e5e45a2ae69c7e5cf5fce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.14453125, 'eval_runtime': 12.7622, 'eval_samples_per_second': 94.811, 'eval_steps_per_second': 11.91, 'epoch': 1.0}\n",
      "{'loss': 12.7814, 'grad_norm': 2.542156934738159, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.06}\n",
      "{'loss': 12.7792, 'grad_norm': 2.521373987197876, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 12.8394, 'grad_norm': 2.530184507369995, 'learning_rate': 1.8e-05, 'epoch': 1.19}\n",
      "{'loss': 12.5987, 'grad_norm': 2.516324996948242, 'learning_rate': 1.9e-05, 'epoch': 1.26}\n",
      "{'loss': 12.8314, 'grad_norm': 2.555100440979004, 'learning_rate': 2e-05, 'epoch': 1.32}\n",
      "{'loss': 12.6418, 'grad_norm': 2.4924168586730957, 'learning_rate': 2.1e-05, 'epoch': 1.39}\n",
      "{'loss': 12.6099, 'grad_norm': 2.5245957374572754, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.45}\n",
      "{'loss': 12.7792, 'grad_norm': 2.5380361080169678, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.52}\n",
      "{'loss': 12.8683, 'grad_norm': 2.522397518157959, 'learning_rate': 2.4e-05, 'epoch': 1.59}\n",
      "{'loss': 12.5159, 'grad_norm': 2.5101232528686523, 'learning_rate': 2.5e-05, 'epoch': 1.65}\n",
      "{'loss': 12.631, 'grad_norm': 2.5142159461975098, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.72}\n",
      "{'loss': 12.711, 'grad_norm': 2.5390405654907227, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.79}\n",
      "{'loss': 12.7673, 'grad_norm': 2.525249719619751, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.85}\n",
      "{'loss': 12.6479, 'grad_norm': 2.522282361984253, 'learning_rate': 2.9e-05, 'epoch': 1.92}\n",
      "{'loss': 12.848, 'grad_norm': 2.533879280090332, 'learning_rate': 3e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65019778b7541ddb27bce6ae4687f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1396484375, 'eval_runtime': 12.8637, 'eval_samples_per_second': 94.063, 'eval_steps_per_second': 11.816, 'epoch': 2.0}\n",
      "{'loss': 12.6816, 'grad_norm': 2.5027105808258057, 'learning_rate': 3.1e-05, 'epoch': 2.05}\n",
      "{'loss': 12.6751, 'grad_norm': 2.535924196243286, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.12}\n",
      "{'loss': 12.7852, 'grad_norm': 2.5418529510498047, 'learning_rate': 3.3e-05, 'epoch': 2.18}\n",
      "{'loss': 12.7766, 'grad_norm': 2.5218050479888916, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.25}\n",
      "{'loss': 12.5273, 'grad_norm': 2.5364718437194824, 'learning_rate': 3.5e-05, 'epoch': 2.31}\n",
      "{'loss': 12.8865, 'grad_norm': 2.525193452835083, 'learning_rate': 3.6e-05, 'epoch': 2.38}\n",
      "{'loss': 12.8067, 'grad_norm': 2.5220561027526855, 'learning_rate': 3.7e-05, 'epoch': 2.45}\n",
      "{'loss': 12.7215, 'grad_norm': 2.5229389667510986, 'learning_rate': 3.8e-05, 'epoch': 2.51}\n",
      "{'loss': 12.8121, 'grad_norm': 2.535749912261963, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.58}\n",
      "{'loss': 12.4982, 'grad_norm': 2.5169527530670166, 'learning_rate': 4e-05, 'epoch': 2.64}\n",
      "{'loss': 12.5676, 'grad_norm': 2.5245306491851807, 'learning_rate': 4.1e-05, 'epoch': 2.71}\n",
      "{'loss': 12.7208, 'grad_norm': 2.523040294647217, 'learning_rate': 4.2e-05, 'epoch': 2.78}\n",
      "{'loss': 12.4118, 'grad_norm': 2.5550036430358887, 'learning_rate': 4.3e-05, 'epoch': 2.84}\n",
      "{'loss': 12.5463, 'grad_norm': 2.545595169067383, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.91}\n",
      "{'loss': 12.8176, 'grad_norm': 2.5500705242156982, 'learning_rate': 4.5e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f205f23b694695bfa5a199f7075bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1318359375, 'eval_runtime': 12.896, 'eval_samples_per_second': 93.827, 'eval_steps_per_second': 11.787, 'epoch': 3.0}\n",
      "{'loss': 12.7827, 'grad_norm': 2.5803093910217285, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.04}\n",
      "{'loss': 12.5722, 'grad_norm': 2.5200836658477783, 'learning_rate': 4.7e-05, 'epoch': 3.11}\n",
      "{'loss': 12.7497, 'grad_norm': 2.5125348567962646, 'learning_rate': 4.8e-05, 'epoch': 3.17}\n",
      "{'loss': 12.8357, 'grad_norm': 2.540024995803833, 'learning_rate': 4.9e-05, 'epoch': 3.24}\n",
      "{'loss': 12.4994, 'grad_norm': 2.51886248588562, 'learning_rate': 5e-05, 'epoch': 3.31}\n",
      "{'loss': 12.8263, 'grad_norm': 2.541128396987915, 'learning_rate': 4.803921568627452e-05, 'epoch': 3.37}\n",
      "{'loss': 12.6702, 'grad_norm': 2.5528221130371094, 'learning_rate': 4.607843137254902e-05, 'epoch': 3.44}\n",
      "{'loss': 12.788, 'grad_norm': 2.541590690612793, 'learning_rate': 4.411764705882353e-05, 'epoch': 3.5}\n",
      "{'loss': 12.8692, 'grad_norm': 2.5243353843688965, 'learning_rate': 4.215686274509804e-05, 'epoch': 3.57}\n",
      "{'loss': 12.604, 'grad_norm': 2.566432476043701, 'learning_rate': 4.0196078431372555e-05, 'epoch': 3.64}\n",
      "{'loss': 12.6494, 'grad_norm': 2.555530548095703, 'learning_rate': 3.8235294117647055e-05, 'epoch': 3.7}\n",
      "{'loss': 12.6477, 'grad_norm': 2.57755708694458, 'learning_rate': 3.627450980392157e-05, 'epoch': 3.77}\n",
      "{'loss': 12.5046, 'grad_norm': 2.5502800941467285, 'learning_rate': 3.431372549019608e-05, 'epoch': 3.83}\n",
      "{'loss': 12.6723, 'grad_norm': 2.549494743347168, 'learning_rate': 3.235294117647059e-05, 'epoch': 3.9}\n",
      "{'loss': 12.627, 'grad_norm': 2.586857795715332, 'learning_rate': 3.0392156862745097e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be05388fe66b4fcf85f99fe83b251116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1236982345581055, 'eval_runtime': 12.9211, 'eval_samples_per_second': 93.646, 'eval_steps_per_second': 11.764, 'epoch': 4.0}\n",
      "{'loss': 12.7124, 'grad_norm': 2.5218353271484375, 'learning_rate': 2.8431372549019608e-05, 'epoch': 4.03}\n",
      "{'loss': 12.6232, 'grad_norm': 2.5101404190063477, 'learning_rate': 2.647058823529412e-05, 'epoch': 4.1}\n",
      "{'loss': 12.8008, 'grad_norm': 2.557892084121704, 'learning_rate': 2.4509803921568626e-05, 'epoch': 4.17}\n",
      "{'loss': 12.7783, 'grad_norm': 2.5214688777923584, 'learning_rate': 2.2549019607843138e-05, 'epoch': 4.23}\n",
      "{'loss': 12.6061, 'grad_norm': 2.51123309135437, 'learning_rate': 2.058823529411765e-05, 'epoch': 4.3}\n",
      "{'loss': 12.6902, 'grad_norm': 2.5596652030944824, 'learning_rate': 1.862745098039216e-05, 'epoch': 4.36}\n",
      "{'loss': 12.7961, 'grad_norm': 2.531599521636963, 'learning_rate': 1.6666666666666667e-05, 'epoch': 4.43}\n",
      "{'loss': 12.5518, 'grad_norm': 2.5393829345703125, 'learning_rate': 1.4705882352941177e-05, 'epoch': 4.5}\n",
      "{'loss': 12.7058, 'grad_norm': 2.531825542449951, 'learning_rate': 1.2745098039215686e-05, 'epoch': 4.56}\n",
      "{'loss': 12.8394, 'grad_norm': 2.5514042377471924, 'learning_rate': 1.0784313725490197e-05, 'epoch': 4.63}\n",
      "{'loss': 12.4853, 'grad_norm': 2.5116961002349854, 'learning_rate': 8.823529411764707e-06, 'epoch': 4.69}\n",
      "{'loss': 12.5146, 'grad_norm': 2.5647337436676025, 'learning_rate': 6.862745098039216e-06, 'epoch': 4.76}\n",
      "{'loss': 12.7267, 'grad_norm': 2.5343968868255615, 'learning_rate': 4.901960784313726e-06, 'epoch': 4.83}\n",
      "{'loss': 12.7182, 'grad_norm': 2.4891626834869385, 'learning_rate': 2.9411764705882355e-06, 'epoch': 4.89}\n",
      "{'loss': 12.6019, 'grad_norm': 2.5448474884033203, 'learning_rate': 9.80392156862745e-07, 'epoch': 4.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c998d883c2489899747196486ed345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1194658279418945, 'eval_runtime': 12.8731, 'eval_samples_per_second': 93.994, 'eval_steps_per_second': 11.808, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_loss.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 362.0036, 'train_samples_per_second': 66.823, 'train_steps_per_second': 2.086, 'train_loss': 12.700960424562163, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=755, training_loss=12.700960424562163, metrics={'train_runtime': 362.0036, 'train_samples_per_second': 66.823, 'train_steps_per_second': 2.086, 'total_flos': 3.0373613146251264e+16, 'train_loss': 12.700960424562163, 'epoch': 4.991735537190083})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "print(\"\\nIniciando entrenamiento para generación de secuencias...\")\n",
    "print(\"Este modelo aprenderá a generar secuencias de proteínas similares a las del entrenamiento\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e33f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de generación guardado en ./prot_xlnet_generation_finetuned_cancer\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Modelo de generación guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9bc1885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8662111a927e4bfba0db132ba6f1aaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación: {'eval_loss': 4.1194658279418945, 'eval_runtime': 12.7203, 'eval_samples_per_second': 95.124, 'eval_steps_per_second': 11.949, 'epoch': 4.991735537190083}\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "print(\"Evaluando el modelo...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Resultados de evaluación: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23adff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para generar nuevas secuencias\n",
    "def generate_protein_sequence(prompt=\"\", max_new_tokens=100, temperature=0.8, do_sample=True, repetition_penalty = 1.1):\n",
    "    \"\"\"\n",
    "    Genera una nueva secuencia de proteína.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Secuencia inicial (puede estar vacía)\n",
    "        max_new_tokens: Número máximo de tokens nuevos a generar\n",
    "        temperature: Controla la aleatoriedad (más alto = más aleatorio)\n",
    "        do_sample: Si usar sampling o greedy decoding\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Si no hay prompt, usar un token especial o secuencia corta común\n",
    "    if not prompt:\n",
    "        prompt = \"M\"  # Muchas proteínas empiezan con metionina\n",
    "    \n",
    "    # Tokenizar el prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generar secuencia\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty= repetition_penalty #1.1\n",
    "        )\n",
    "    \n",
    "    # Decodificar la secuencia generada\n",
    "    generated_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remover el prompt original para obtener solo la parte generada\n",
    "    if prompt:\n",
    "        generated_sequence = generated_sequence[len(prompt):]\n",
    "    \n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d039739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_sequences(num_sequences=5, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    Genera múltiples secuencias de proteínas.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        seq = generate_protein_sequence(**generation_kwargs)\n",
    "        sequences.append(seq)\n",
    "        print(f\"Secuencia {i+1}: {seq}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb06e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EJEMPLOS DE GENERACIÓN DE SECUENCIAS\n",
      "==================================================\n",
      "\n",
      "1. Generación sin prompt inicial:\n",
      "Secuencia 1: ()) W H–\" S\n",
      "Secuencia 2:  Q P€ F-– V£.\n",
      "Secuencia 3: € T\"\n",
      "Secuencia 4:  A A G K Y N\" TX K L K A D R V Y\n",
      "Secuencia 5: - D\n",
      "Secuencia 6:  V\"( R. S€ G T Q) T£\n",
      "Secuencia 7:  Q\n",
      "Secuencia 8:  N T D C\"£ A P( A–\n",
      "Secuencia 9: £€-)£. G(–\n",
      "Secuencia 10: . G D Y T-\n",
      "Secuencia 11: \n",
      "Secuencia 12:  W.\"€ C- N£. P–€\n",
      "Secuencia 13: \n",
      "Secuencia 14:  A G€ R(()€ P– F\" T R\n",
      "Secuencia 15: X A K\n",
      "Secuencia 16: -( Y£\" S\n",
      "Secuencia 17: .(\"–)..€\n",
      "Secuencia 18:  T G–\".(££ A€\n",
      "Secuencia 19: € S D-X G T\n",
      "Secuencia 20: .\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de generación\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EJEMPLOS DE GENERACIÓN DE SECUENCIAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generar secuencias sin prompt\n",
    "print(\"\\n1. Generación sin prompt inicial:\")\n",
    "generated_seqs = generate_multiple_sequences(\n",
    "    num_sequences=20,\n",
    "    max_new_tokens=17,\n",
    "    temperature=3.0,\n",
    "    repetition_penalty = 5.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7507db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Generación con diferentes temperaturas:\n",
      "\n",
      "Temperatura 1.0:\n",
      "   N I V S Q A A A I R V N S V I S Q T P N T R I N S N I S R V L F Q M Q Y K G Q Q N P\n",
      "\n",
      "Temperatura 1.2:\n",
      "  \n",
      "\n",
      "Temperatura 1.5:\n",
      "  \n",
      "\n",
      "Temperatura 2.0:\n",
      "  \n",
      "\n",
      "Temperatura 2.5:\n",
      "  E I L R M M I F F S V Y L R T N H P S V T D G W W M W I K E I G T A T S M A S R G F\n",
      "\n",
      "Temperatura 3.0:\n",
      "  P L V P F E P T I P Q R L Y H A S A F V P E M R N E A S M A R T M A F S D S G P K I E S\n"
     ]
    }
   ],
   "source": [
    "# Generar con diferentes temperaturas\n",
    "print(\"\\n2. Generación con diferentes temperaturas:\")\n",
    "for temp in [1.0, 1.2, 1.5, 2.0, 2.5, 3.0]:\n",
    "    print(f\"\\nTemperatura {temp}:\")\n",
    "    seq = generate_protein_sequence(\n",
    "        prompt=\"AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW\",\n",
    "        max_new_tokens=60,\n",
    "        temperature=temp,\n",
    "         repetition_penalty = 5.0\n",
    "\n",
    "    )\n",
    "    print(f\"  {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "181a6c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Generación con prompt específico:\n",
      "Prompt: AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW\n",
      "Generado: C R-\n"
     ]
    }
   ],
   "source": [
    "# Generar con prompt específico\n",
    "print(\"\\n3. Generación con prompt específico:\")\n",
    "prompt = 'AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW'\n",
    "extended_seq = generate_protein_sequence(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=2.0,\n",
    "    repetition_penalty = 5.0\n",
    "    )\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generado: {extended_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97d5166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:  E D D Y S E S W Y H S A S M N V D G V L S V P P S Q P K R K C Y N N H Y W R Q K S I F K I G N I V Q\n",
      "Secuencia 2:  N K S P S C P D Q D T L Q P P I K M A M E L S E H H K L T I N S Y K F Q E T R H F I V T Q S K S N G\n",
      "Secuencia 3:  P T I F T T Y W I F R Y F V Y F W F P F I P T C V I Y H Y S L V R G H K C A A W K Y V C A T V W C W\n",
      "Secuencia 4:  K N N R C N R R M N N V G W P C S C T V I V L V V A E Y F L I T T I Q E S M R F W S V R C V V G M T\n",
      "Secuencia 5:  D L E Q P F E A Y Q M M R G R G S G F A Y N Y L C S M T N F G P V T N Y Q V V V S G G N W M K R W M\n",
      "Secuencia 6:  S W D DX N I T K Y P H K Q L E Y W G N T W D V P R S N S E L N S L M D W A L V I D C Q Q M I V I E\n",
      "Secuencia 7:  N C P E A G K Y C Q S D S C F I Q T M M D F G S N T A E D C A T F Q D D A F F F G G P G S S L R K D\n",
      "Secuencia 8:  S K N C H P H L G RX W Q K M Q G E Q W T L A L M D T T T P K L M V R L S L T F W E R V K G D S G C\n",
      "Secuencia 9:  L F V W W W G P R A K V C N L V D V K L G S F Y N K A F F I K G I Y V L M V K I I Y E I L N F V V T\n",
      "Secuencia 10:  E VX T A Q A V P I I E E N T P A L E E P P R C W E W A F E Y E Y G W G Q N P Y F D R G W Q H H L D\n",
      "Secuencia 11:  T T K R R W P W E G E K H C H F M W F S C N K P A W E K K G L V Y T D V T C P T D N I I N I S K A N\n",
      "Secuencia 12:  K V T R K G G K M T R Q I A Y I K V E Y N E S D A Y N A L G N G R I S P L T T F H S A E K Y D G G L\n",
      "Secuencia 13:  L W L E Y F P Q C W Y M G S D T W Y P Q D D Y T P I H V P S L F W E I P I I A T T S I F W N Y S E\n",
      "Secuencia 14:  V M V M M T S L H L I S Y L P M Y L Y T Y E Y L T V L V A F Y E T I M V P Y C F P C M F T I N W T C\n",
      "Secuencia 15:  E D Y G N M G S K M G Q T V R T E Q Q F R Y S H P K Q V N I E P I A F H L R H N R R Y C H H S N H G\n",
      "Secuencia 16:  K L W K Q Q N R R W W S A E W Q S S M L K A S S A M I P E P R V R L W I E H R F Q T Y L I I W A V M\n",
      "Secuencia 17:  A N V Y A M M S F N I M G N Y A V H R L E R Y N H A G R R M F H K M S A V G K L H E T N I I R M S W\n",
      "Secuencia 18:  E C E V IX Q Q Q K D E H K R S Y S C N W K I W G H K W G T W E I I E C E Y I F I K D N Y K D W I T\n",
      "Secuencia 19:  D I G A Y W E E V D E F Y V E T K P D P K H W W N E S P P E F Q V Q A L S G V P W N F P K K L F M C\n",
      "Secuencia 20:  V I V D N E I A M D E R R C D F P S W A W I L A I N R Y G S W W D N V E W I I S R Q F K K S Y W S W\n"
     ]
    }
   ],
   "source": [
    "generated_seqs2 = generate_multiple_sequences(\n",
    "    num_sequences=20,\n",
    "    max_new_tokens=50,\n",
    "    temperature=2.0,\n",
    "    prompt = 'A A V A L L P A V L L A L L A P Q L G K K K H R R R P S K K K R H W',\n",
    "    repetition_penalty = 5.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad5e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
