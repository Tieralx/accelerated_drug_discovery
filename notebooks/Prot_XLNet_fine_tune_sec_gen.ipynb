{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43334558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aae9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar wandb completamente\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a3d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available devices: 1\n",
      "current device: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'available devices: {torch.cuda.device_count()}')\n",
    "print(f'current device: { torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0b790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay GPU disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f49430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de par√°metros para generaci√≥n de secuencias\n",
    "model_name = \"Rostlab/prot_xlnet\"\n",
    "batch_size = 8  # Reducido para generaci√≥n que requiere m√°s memoria\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "max_length = 512\n",
    "output_dir = \"./prot_xlnet_generation_finetuned_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5db36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida si no existe\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466589d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando tokenizador y modelo para generaci√≥n de secuencias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alejandros\\Documents\\GitHub\\Advanced_drug_discovery\\.venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "# Cargar tokenizador y modelo para generaci√≥n de lenguaje\n",
    "print(\"Cargando tokenizador y modelo para generaci√≥n de secuencias...\")\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "# Usar XLNetLMHeadModel para generaci√≥n de texto/secuencias\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f488d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario del tokenizador: 37 tokens\n",
      "El modelo est√° configurado para generaci√≥n de secuencias de prote√≠nas\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulario del tokenizador: {tokenizer.vocab_size} tokens\")\n",
    "print(\"El modelo est√° configurado para generaci√≥n de secuencias de prote√≠nas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7f0db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetLMHeadModel(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(37, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0-29): 30 x XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): ReLU()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_loss): Linear(in_features=1024, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mover el modelo al dispositivo adecuado\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fecc8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para congelar las capas base y solo entrenar las nuevas\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da424da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para crear datos de ejemplo (reemplazar con secuencias reales)\n",
    "def create_protein_sequences_data():\n",
    "    \"\"\"\n",
    "    Crea un conjunto de datos de ejemplo de secuencias de prote√≠nas.\n",
    "    REEMPLAZA ESTA FUNCI√ìN con la carga de tus secuencias reales.\n",
    "    \"\"\"\n",
    "    # Secuencias de ejemplo m√°s realistas\n",
    "    sequences = [\n",
    "        \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "        \"MASNTVSAQGGSNRPVRDLASRQDFVRASSIIEKQLRDKVSADDLPVTLAQHLAVNFLHVLRLLE\",\n",
    "        \"MTMDKSELVQKAKLAEQAERYDDMAAAMKAVTEQGHELSNEERNLLSVAYKNVVGARRSSWRVVSSIEQKTEGAEKKQQ\",\n",
    "        \"MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDLQAKGIQYLHQKGLKKQVPEGVKVP\",\n",
    "        \"MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNLHGLFGRKTGQAPGYSYTAANKNKGIIWGEDTLMEYLENPKKYIPGTKMIFVGIKKKEERADLIAYLKKATNE\"\n",
    "    ]\n",
    "    \n",
    "    # A√±adir m√°s secuencias simuladas si necesitas m√°s datos de prueba\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    for i in range(50):  # Reducido para este ejemplo\n",
    "        seq_length = np.random.randint(80, 300)  # Longitudes m√°s realistas\n",
    "        seq = ''.join(np.random.choice(list(amino_acids)) for _ in range(seq_length))\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b581f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_protein_sequences(file_path=None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para cargar tus secuencias reales de prote√≠nas.\n",
    "    Adapta esta funci√≥n seg√∫n el formato de tus datos.\n",
    "    \n",
    "    Formatos soportados:\n",
    "    - CSV con columna 'sequence'\n",
    "    - FASTA\n",
    "    - Texto plano (una secuencia por l√≠nea)\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        print(\"Usando datos de ejemplo. Para usar tus datos reales:\")\n",
    "        print(\"1. Modifica la funci√≥n load_real_protein_sequences()\")\n",
    "        print(\"2. O pasa el path a tu archivo de secuencias\")\n",
    "        return create_protein_sequences_data()\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'sequence' in df.columns:\n",
    "            sequences = df['sequence'].tolist()\n",
    "        else:\n",
    "            print(\"El CSV debe tener una columna llamada 'sequence'\")\n",
    "            return create_protein_sequences_data()\n",
    "    \n",
    "    elif file_path.endswith('.fasta') or file_path.endswith('.fa'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            current_seq = \"\"\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                        current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line.strip()\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "    \n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            sequences = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Cargadas {len(sequences)} secuencias de {file_path}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0fa0fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargadas 6048 secuencias de ../data/raw/peptides_cancer.csv\n",
      "Total de secuencias cargadas: 6048\n",
      "Longitud promedio: 17.3 amino√°cidos\n",
      "Rango de longitudes: 2 - 97\n"
     ]
    }
   ],
   "source": [
    "# Cargar secuencias (modifica el path para usar tus datos reales)\n",
    "#sequences = load_real_protein_sequences()  # Cambia por \n",
    "sequences = load_real_protein_sequences(\"../data/raw/peptides_cancer.csv\")\n",
    "\n",
    "print(f\"Total de secuencias cargadas: {len(sequences)}\")\n",
    "print(f\"Longitud promedio: {np.mean([len(seq) for seq in sequences]):.1f} amino√°cidos\")\n",
    "print(f\"Rango de longitudes: {min(len(seq) for seq in sequences)} - {max(len(seq) for seq in sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9667526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjuntos de entrenamiento y validaci√≥n\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25798212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para tokenizar las secuencias para generaci√≥n\n",
    "def tokenize_for_generation(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza las secuencias para entrenamiento de generaci√≥n de lenguaje.\n",
    "    A√±ade tokens especiales y prepara para masked language modeling.\n",
    "    \"\"\"\n",
    "    # Tokenizar las secuencias\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    \n",
    "    # Para XLNet, usamos los input_ids como labels tambi√©n (language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae9653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets\n",
    "train_dataset = Dataset.from_dict({\"sequence\": train_sequences})\n",
    "val_dataset = Dataset.from_dict({\"sequence\": val_sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc50c773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d210c2ca8e4dd59d94ac4a9ec6a678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda2f8797af045a199d5ea554036a114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizar los datasets\n",
    "train_dataset = train_dataset.map(tokenize_for_generation, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_for_generation, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d614b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el data collator para language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # XLNet usa autoregressive LM, no masked LM\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdcc305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alejandros\\Documents\\GitHub\\Advanced_drug_discovery\\.venv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=4,  # Aumentado para compensar batch size menor\n",
    "    report_to=None,\n",
    "    disable_tqdm=False,\n",
    "    prediction_loss_only=True,  # Solo calculamos loss para generaci√≥n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ccc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funci√≥n de m√©tricas para generaci√≥n\n",
    "def compute_generation_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas espec√≠ficas para generaci√≥n de secuencias.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Para generaci√≥n, nos enfocamos principalmente en la perplejidad (calculada del loss)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d039e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquitectura del modelo:\n",
      "- Tipo: XLNetLMHeadModel (para generaci√≥n de secuencias)\n",
      "- Par√°metros: 409,413,669\n",
      "- Vocabulario: 37 tokens\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el Trainer para generaci√≥n\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_generation_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nArquitectura del modelo:\")\n",
    "print(f\"- Tipo: {type(model).__name__} (para generaci√≥n de secuencias)\")\n",
    "print(f\"- Par√°metros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"- Vocabulario: {tokenizer.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9cf2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento para generaci√≥n de secuencias...\n",
      "Este modelo aprender√° a generar secuencias de prote√≠nas similares a las del entrenamiento\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3281e60ddc4dce83f31ed65bfeed1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.6694, 'grad_norm': 2.5340099334716797, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 12.7472, 'grad_norm': 2.5389554500579834, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.13}\n",
      "{'loss': 12.6117, 'grad_norm': 2.534273862838745, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 12.744, 'grad_norm': 2.530470848083496, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.26}\n",
      "{'loss': 12.798, 'grad_norm': 2.5221543312072754, 'learning_rate': 5e-06, 'epoch': 0.33}\n",
      "{'loss': 12.7354, 'grad_norm': 2.526870012283325, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 12.6937, 'grad_norm': 2.5525267124176025, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.46}\n",
      "{'loss': 12.7087, 'grad_norm': 2.5521256923675537, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.53}\n",
      "{'loss': 12.8252, 'grad_norm': 2.52350115776062, 'learning_rate': 9e-06, 'epoch': 0.6}\n",
      "{'loss': 12.795, 'grad_norm': 2.5364279747009277, 'learning_rate': 1e-05, 'epoch': 0.66}\n",
      "{'loss': 12.7503, 'grad_norm': 2.531712293624878, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.73}\n",
      "{'loss': 12.722, 'grad_norm': 2.5665271282196045, 'learning_rate': 1.2e-05, 'epoch': 0.79}\n",
      "{'loss': 12.8238, 'grad_norm': 2.5247340202331543, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.86}\n",
      "{'loss': 12.8905, 'grad_norm': 2.5279407501220703, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.93}\n",
      "{'loss': 12.6654, 'grad_norm': 2.529435634613037, 'learning_rate': 1.5e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1ac753e5e45a2ae69c7e5cf5fce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.14453125, 'eval_runtime': 12.7622, 'eval_samples_per_second': 94.811, 'eval_steps_per_second': 11.91, 'epoch': 1.0}\n",
      "{'loss': 12.7814, 'grad_norm': 2.542156934738159, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.06}\n",
      "{'loss': 12.7792, 'grad_norm': 2.521373987197876, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 12.8394, 'grad_norm': 2.530184507369995, 'learning_rate': 1.8e-05, 'epoch': 1.19}\n",
      "{'loss': 12.5987, 'grad_norm': 2.516324996948242, 'learning_rate': 1.9e-05, 'epoch': 1.26}\n",
      "{'loss': 12.8314, 'grad_norm': 2.555100440979004, 'learning_rate': 2e-05, 'epoch': 1.32}\n",
      "{'loss': 12.6418, 'grad_norm': 2.4924168586730957, 'learning_rate': 2.1e-05, 'epoch': 1.39}\n",
      "{'loss': 12.6099, 'grad_norm': 2.5245957374572754, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.45}\n",
      "{'loss': 12.7792, 'grad_norm': 2.5380361080169678, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.52}\n",
      "{'loss': 12.8683, 'grad_norm': 2.522397518157959, 'learning_rate': 2.4e-05, 'epoch': 1.59}\n",
      "{'loss': 12.5159, 'grad_norm': 2.5101232528686523, 'learning_rate': 2.5e-05, 'epoch': 1.65}\n",
      "{'loss': 12.631, 'grad_norm': 2.5142159461975098, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.72}\n",
      "{'loss': 12.711, 'grad_norm': 2.5390405654907227, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.79}\n",
      "{'loss': 12.7673, 'grad_norm': 2.525249719619751, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.85}\n",
      "{'loss': 12.6479, 'grad_norm': 2.522282361984253, 'learning_rate': 2.9e-05, 'epoch': 1.92}\n",
      "{'loss': 12.848, 'grad_norm': 2.533879280090332, 'learning_rate': 3e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65019778b7541ddb27bce6ae4687f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1396484375, 'eval_runtime': 12.8637, 'eval_samples_per_second': 94.063, 'eval_steps_per_second': 11.816, 'epoch': 2.0}\n",
      "{'loss': 12.6816, 'grad_norm': 2.5027105808258057, 'learning_rate': 3.1e-05, 'epoch': 2.05}\n",
      "{'loss': 12.6751, 'grad_norm': 2.535924196243286, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.12}\n",
      "{'loss': 12.7852, 'grad_norm': 2.5418529510498047, 'learning_rate': 3.3e-05, 'epoch': 2.18}\n",
      "{'loss': 12.7766, 'grad_norm': 2.5218050479888916, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.25}\n",
      "{'loss': 12.5273, 'grad_norm': 2.5364718437194824, 'learning_rate': 3.5e-05, 'epoch': 2.31}\n",
      "{'loss': 12.8865, 'grad_norm': 2.525193452835083, 'learning_rate': 3.6e-05, 'epoch': 2.38}\n",
      "{'loss': 12.8067, 'grad_norm': 2.5220561027526855, 'learning_rate': 3.7e-05, 'epoch': 2.45}\n",
      "{'loss': 12.7215, 'grad_norm': 2.5229389667510986, 'learning_rate': 3.8e-05, 'epoch': 2.51}\n",
      "{'loss': 12.8121, 'grad_norm': 2.535749912261963, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.58}\n",
      "{'loss': 12.4982, 'grad_norm': 2.5169527530670166, 'learning_rate': 4e-05, 'epoch': 2.64}\n",
      "{'loss': 12.5676, 'grad_norm': 2.5245306491851807, 'learning_rate': 4.1e-05, 'epoch': 2.71}\n",
      "{'loss': 12.7208, 'grad_norm': 2.523040294647217, 'learning_rate': 4.2e-05, 'epoch': 2.78}\n",
      "{'loss': 12.4118, 'grad_norm': 2.5550036430358887, 'learning_rate': 4.3e-05, 'epoch': 2.84}\n",
      "{'loss': 12.5463, 'grad_norm': 2.545595169067383, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.91}\n",
      "{'loss': 12.8176, 'grad_norm': 2.5500705242156982, 'learning_rate': 4.5e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f205f23b694695bfa5a199f7075bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1318359375, 'eval_runtime': 12.896, 'eval_samples_per_second': 93.827, 'eval_steps_per_second': 11.787, 'epoch': 3.0}\n",
      "{'loss': 12.7827, 'grad_norm': 2.5803093910217285, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.04}\n",
      "{'loss': 12.5722, 'grad_norm': 2.5200836658477783, 'learning_rate': 4.7e-05, 'epoch': 3.11}\n",
      "{'loss': 12.7497, 'grad_norm': 2.5125348567962646, 'learning_rate': 4.8e-05, 'epoch': 3.17}\n",
      "{'loss': 12.8357, 'grad_norm': 2.540024995803833, 'learning_rate': 4.9e-05, 'epoch': 3.24}\n",
      "{'loss': 12.4994, 'grad_norm': 2.51886248588562, 'learning_rate': 5e-05, 'epoch': 3.31}\n",
      "{'loss': 12.8263, 'grad_norm': 2.541128396987915, 'learning_rate': 4.803921568627452e-05, 'epoch': 3.37}\n",
      "{'loss': 12.6702, 'grad_norm': 2.5528221130371094, 'learning_rate': 4.607843137254902e-05, 'epoch': 3.44}\n",
      "{'loss': 12.788, 'grad_norm': 2.541590690612793, 'learning_rate': 4.411764705882353e-05, 'epoch': 3.5}\n",
      "{'loss': 12.8692, 'grad_norm': 2.5243353843688965, 'learning_rate': 4.215686274509804e-05, 'epoch': 3.57}\n",
      "{'loss': 12.604, 'grad_norm': 2.566432476043701, 'learning_rate': 4.0196078431372555e-05, 'epoch': 3.64}\n",
      "{'loss': 12.6494, 'grad_norm': 2.555530548095703, 'learning_rate': 3.8235294117647055e-05, 'epoch': 3.7}\n",
      "{'loss': 12.6477, 'grad_norm': 2.57755708694458, 'learning_rate': 3.627450980392157e-05, 'epoch': 3.77}\n",
      "{'loss': 12.5046, 'grad_norm': 2.5502800941467285, 'learning_rate': 3.431372549019608e-05, 'epoch': 3.83}\n",
      "{'loss': 12.6723, 'grad_norm': 2.549494743347168, 'learning_rate': 3.235294117647059e-05, 'epoch': 3.9}\n",
      "{'loss': 12.627, 'grad_norm': 2.586857795715332, 'learning_rate': 3.0392156862745097e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be05388fe66b4fcf85f99fe83b251116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1236982345581055, 'eval_runtime': 12.9211, 'eval_samples_per_second': 93.646, 'eval_steps_per_second': 11.764, 'epoch': 4.0}\n",
      "{'loss': 12.7124, 'grad_norm': 2.5218353271484375, 'learning_rate': 2.8431372549019608e-05, 'epoch': 4.03}\n",
      "{'loss': 12.6232, 'grad_norm': 2.5101404190063477, 'learning_rate': 2.647058823529412e-05, 'epoch': 4.1}\n",
      "{'loss': 12.8008, 'grad_norm': 2.557892084121704, 'learning_rate': 2.4509803921568626e-05, 'epoch': 4.17}\n",
      "{'loss': 12.7783, 'grad_norm': 2.5214688777923584, 'learning_rate': 2.2549019607843138e-05, 'epoch': 4.23}\n",
      "{'loss': 12.6061, 'grad_norm': 2.51123309135437, 'learning_rate': 2.058823529411765e-05, 'epoch': 4.3}\n",
      "{'loss': 12.6902, 'grad_norm': 2.5596652030944824, 'learning_rate': 1.862745098039216e-05, 'epoch': 4.36}\n",
      "{'loss': 12.7961, 'grad_norm': 2.531599521636963, 'learning_rate': 1.6666666666666667e-05, 'epoch': 4.43}\n",
      "{'loss': 12.5518, 'grad_norm': 2.5393829345703125, 'learning_rate': 1.4705882352941177e-05, 'epoch': 4.5}\n",
      "{'loss': 12.7058, 'grad_norm': 2.531825542449951, 'learning_rate': 1.2745098039215686e-05, 'epoch': 4.56}\n",
      "{'loss': 12.8394, 'grad_norm': 2.5514042377471924, 'learning_rate': 1.0784313725490197e-05, 'epoch': 4.63}\n",
      "{'loss': 12.4853, 'grad_norm': 2.5116961002349854, 'learning_rate': 8.823529411764707e-06, 'epoch': 4.69}\n",
      "{'loss': 12.5146, 'grad_norm': 2.5647337436676025, 'learning_rate': 6.862745098039216e-06, 'epoch': 4.76}\n",
      "{'loss': 12.7267, 'grad_norm': 2.5343968868255615, 'learning_rate': 4.901960784313726e-06, 'epoch': 4.83}\n",
      "{'loss': 12.7182, 'grad_norm': 2.4891626834869385, 'learning_rate': 2.9411764705882355e-06, 'epoch': 4.89}\n",
      "{'loss': 12.6019, 'grad_norm': 2.5448474884033203, 'learning_rate': 9.80392156862745e-07, 'epoch': 4.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c998d883c2489899747196486ed345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1194658279418945, 'eval_runtime': 12.8731, 'eval_samples_per_second': 93.994, 'eval_steps_per_second': 11.808, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_loss.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 362.0036, 'train_samples_per_second': 66.823, 'train_steps_per_second': 2.086, 'train_loss': 12.700960424562163, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=755, training_loss=12.700960424562163, metrics={'train_runtime': 362.0036, 'train_samples_per_second': 66.823, 'train_steps_per_second': 2.086, 'total_flos': 3.0373613146251264e+16, 'train_loss': 12.700960424562163, 'epoch': 4.991735537190083})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "print(\"\\nIniciando entrenamiento para generaci√≥n de secuencias...\")\n",
    "print(\"Este modelo aprender√° a generar secuencias de prote√≠nas similares a las del entrenamiento\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e33f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de generaci√≥n guardado en ./prot_xlnet_generation_finetuned_cancer\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Modelo de generaci√≥n guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9bc1885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8662111a927e4bfba0db132ba6f1aaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluaci√≥n: {'eval_loss': 4.1194658279418945, 'eval_runtime': 12.7203, 'eval_samples_per_second': 95.124, 'eval_steps_per_second': 11.949, 'epoch': 4.991735537190083}\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "print(\"Evaluando el modelo...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Resultados de evaluaci√≥n: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23adff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para generar nuevas secuencias\n",
    "def generate_protein_sequence(prompt=\"\", max_new_tokens=100, temperature=0.8, do_sample=True, repetition_penalty = 1.1):\n",
    "    \"\"\"\n",
    "    Genera una nueva secuencia de prote√≠na.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Secuencia inicial (puede estar vac√≠a)\n",
    "        max_new_tokens: N√∫mero m√°ximo de tokens nuevos a generar\n",
    "        temperature: Controla la aleatoriedad (m√°s alto = m√°s aleatorio)\n",
    "        do_sample: Si usar sampling o greedy decoding\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Si no hay prompt, usar un token especial o secuencia corta com√∫n\n",
    "    if not prompt:\n",
    "        prompt = \"M\"  # Muchas prote√≠nas empiezan con metionina\n",
    "    \n",
    "    # Tokenizar el prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generar secuencia\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty= repetition_penalty #1.1\n",
    "        )\n",
    "    \n",
    "    # Decodificar la secuencia generada\n",
    "    generated_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remover el prompt original para obtener solo la parte generada\n",
    "    if prompt:\n",
    "        generated_sequence = generated_sequence[len(prompt):]\n",
    "    \n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d039739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_sequences(num_sequences=5, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    Genera m√∫ltiples secuencias de prote√≠nas.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        seq = generate_protein_sequence(**generation_kwargs)\n",
    "        sequences.append(seq)\n",
    "        print(f\"Secuencia {i+1}: {seq}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb06e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EJEMPLOS DE GENERACI√ìN DE SECUENCIAS\n",
      "==================================================\n",
      "\n",
      "1. Generaci√≥n sin prompt inicial:\n",
      "Secuencia 1: ()) W H‚Äì\" S\n",
      "Secuencia 2:  Q P‚Ç¨ F-‚Äì V¬£.\n",
      "Secuencia 3: ‚Ç¨ T\"\n",
      "Secuencia 4:  A A G K Y N\" TX K L K A D R V Y\n",
      "Secuencia 5: - D\n",
      "Secuencia 6:  V\"( R. S‚Ç¨ G T Q) T¬£\n",
      "Secuencia 7:  Q\n",
      "Secuencia 8:  N T D C\"¬£ A P( A‚Äì\n",
      "Secuencia 9: ¬£‚Ç¨-)¬£. G(‚Äì\n",
      "Secuencia 10: . G D Y T-\n",
      "Secuencia 11: \n",
      "Secuencia 12:  W.\"‚Ç¨ C- N¬£. P‚Äì‚Ç¨\n",
      "Secuencia 13: \n",
      "Secuencia 14:  A G‚Ç¨ R(()‚Ç¨ P‚Äì F\" T R\n",
      "Secuencia 15: X A K\n",
      "Secuencia 16: -( Y¬£\" S\n",
      "Secuencia 17: .(\"‚Äì)..‚Ç¨\n",
      "Secuencia 18:  T G‚Äì\".(¬£¬£ A‚Ç¨\n",
      "Secuencia 19: ‚Ç¨ S D-X G T\n",
      "Secuencia 20: .\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de generaci√≥n\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EJEMPLOS DE GENERACI√ìN DE SECUENCIAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generar secuencias sin prompt\n",
    "print(\"\\n1. Generaci√≥n sin prompt inicial:\")\n",
    "generated_seqs = generate_multiple_sequences(\n",
    "    num_sequences=20,\n",
    "    max_new_tokens=17,\n",
    "    temperature=3.0,\n",
    "    repetition_penalty = 5.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7507db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Generaci√≥n con diferentes temperaturas:\n",
      "\n",
      "Temperatura 1.0:\n",
      "   N I V S Q A A A I R V N S V I S Q T P N T R I N S N I S R V L F Q M Q Y K G Q Q N P\n",
      "\n",
      "Temperatura 1.2:\n",
      "  \n",
      "\n",
      "Temperatura 1.5:\n",
      "  \n",
      "\n",
      "Temperatura 2.0:\n",
      "  \n",
      "\n",
      "Temperatura 2.5:\n",
      "  E I L R M M I F F S V Y L R T N H P S V T D G W W M W I K E I G T A T S M A S R G F\n",
      "\n",
      "Temperatura 3.0:\n",
      "  P L V P F E P T I P Q R L Y H A S A F V P E M R N E A S M A R T M A F S D S G P K I E S\n"
     ]
    }
   ],
   "source": [
    "# Generar con diferentes temperaturas\n",
    "print(\"\\n2. Generaci√≥n con diferentes temperaturas:\")\n",
    "for temp in [1.0, 1.2, 1.5, 2.0, 2.5, 3.0]:\n",
    "    print(f\"\\nTemperatura {temp}:\")\n",
    "    seq = generate_protein_sequence(\n",
    "        prompt=\"AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW\",\n",
    "        max_new_tokens=60,\n",
    "        temperature=temp,\n",
    "         repetition_penalty = 5.0\n",
    "\n",
    "    )\n",
    "    print(f\"  {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "181a6c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Generaci√≥n con prompt espec√≠fico:\n",
      "Prompt: AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW\n",
      "Generado: C R-\n"
     ]
    }
   ],
   "source": [
    "# Generar con prompt espec√≠fico\n",
    "print(\"\\n3. Generaci√≥n con prompt espec√≠fico:\")\n",
    "prompt = 'AAVALLPAVLLALLAPQLGKKKHRRRPSKKKRHW'\n",
    "extended_seq = generate_protein_sequence(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=2.0,\n",
    "    repetition_penalty = 5.0\n",
    "    )\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generado: {extended_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97d5166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia 1:  E D D Y S E S W Y H S A S M N V D G V L S V P P S Q P K R K C Y N N H Y W R Q K S I F K I G N I V Q\n",
      "Secuencia 2:  N K S P S C P D Q D T L Q P P I K M A M E L S E H H K L T I N S Y K F Q E T R H F I V T Q S K S N G\n",
      "Secuencia 3:  P T I F T T Y W I F R Y F V Y F W F P F I P T C V I Y H Y S L V R G H K C A A W K Y V C A T V W C W\n",
      "Secuencia 4:  K N N R C N R R M N N V G W P C S C T V I V L V V A E Y F L I T T I Q E S M R F W S V R C V V G M T\n",
      "Secuencia 5:  D L E Q P F E A Y Q M M R G R G S G F A Y N Y L C S M T N F G P V T N Y Q V V V S G G N W M K R W M\n",
      "Secuencia 6:  S W D DX N I T K Y P H K Q L E Y W G N T W D V P R S N S E L N S L M D W A L V I D C Q Q M I V I E\n",
      "Secuencia 7:  N C P E A G K Y C Q S D S C F I Q T M M D F G S N T A E D C A T F Q D D A F F F G G P G S S L R K D\n",
      "Secuencia 8:  S K N C H P H L G RX W Q K M Q G E Q W T L A L M D T T T P K L M V R L S L T F W E R V K G D S G C\n",
      "Secuencia 9:  L F V W W W G P R A K V C N L V D V K L G S F Y N K A F F I K G I Y V L M V K I I Y E I L N F V V T\n",
      "Secuencia 10:  E VX T A Q A V P I I E E N T P A L E E P P R C W E W A F E Y E Y G W G Q N P Y F D R G W Q H H L D\n",
      "Secuencia 11:  T T K R R W P W E G E K H C H F M W F S C N K P A W E K K G L V Y T D V T C P T D N I I N I S K A N\n",
      "Secuencia 12:  K V T R K G G K M T R Q I A Y I K V E Y N E S D A Y N A L G N G R I S P L T T F H S A E K Y D G G L\n",
      "Secuencia 13:  L W L E Y F P Q C W Y M G S D T W Y P Q D D Y T P I H V P S L F W E I P I I A T T S I F W N Y S E\n",
      "Secuencia 14:  V M V M M T S L H L I S Y L P M Y L Y T Y E Y L T V L V A F Y E T I M V P Y C F P C M F T I N W T C\n",
      "Secuencia 15:  E D Y G N M G S K M G Q T V R T E Q Q F R Y S H P K Q V N I E P I A F H L R H N R R Y C H H S N H G\n",
      "Secuencia 16:  K L W K Q Q N R R W W S A E W Q S S M L K A S S A M I P E P R V R L W I E H R F Q T Y L I I W A V M\n",
      "Secuencia 17:  A N V Y A M M S F N I M G N Y A V H R L E R Y N H A G R R M F H K M S A V G K L H E T N I I R M S W\n",
      "Secuencia 18:  E C E V IX Q Q Q K D E H K R S Y S C N W K I W G H K W G T W E I I E C E Y I F I K D N Y K D W I T\n",
      "Secuencia 19:  D I G A Y W E E V D E F Y V E T K P D P K H W W N E S P P E F Q V Q A L S G V P W N F P K K L F M C\n",
      "Secuencia 20:  V I V D N E I A M D E R R C D F P S W A W I L A I N R Y G S W W D N V E W I I S R Q F K K S Y W S W\n"
     ]
    }
   ],
   "source": [
    "generated_seqs2 = generate_multiple_sequences(\n",
    "    num_sequences=20,\n",
    "    max_new_tokens=50,\n",
    "    temperature=2.0,\n",
    "    prompt = 'A A V A L L P A V L L A L L A P Q L G K K K H R R R P S K K K R H W',\n",
    "    repetition_penalty = 5.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad5e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
